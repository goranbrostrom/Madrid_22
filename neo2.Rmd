---
title: "Neonatal models"
author: "Göran Broström"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    citation_package: natbib
    keep_tex: yes
    number_sections: yes
    toc: no
    toc_depth: 2
  bookdown::word_document2:
    toc: no
    toc_depth: 2
  bookdown::html_document2:
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
citation_package: natbib
bibliography: bio.bib
titlepage: no
biblio-style: apalike
documentclass: article
fontsize: 11pt
header-includes:
- \usepackage{a4wide}    
- \usepackage[utf8]{inputenc}
- \usepackage{graphicx}
abstract: "The effect of various covariates  on neonatal mortality in the Umeå region 1895--1950 is studied as an example of how to do it with tabular, anonymised data."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,comment = NA)
```

# Introduction

How to analyse table data with the same statistical models, for instance *Cox regression*,
as you are used to with individual data, is shown here. It helps if you have the 
**R** package *eha* [@eha; @ehar2] for easy conversion of individual data to tabular data
suitable for survival analysis.

# Example: Neonatal mortality in Umeå/Skellefteå, 1895--1950

We use a data set for studying infant mortality from the *Demographic Database*,
Umeå University.

## Read data

Start by reading the raw data file and select a few columns: See Table \@ref(tab:clean).

```{r readdata}
infdat <- readRDS("data/infdat.rds")[, c("id", "enter", "exit", "event", 
                                         "sex", "socStatus", "illeg",  
                                         "parity")]
## Do not remove zero-length spells:
## <- infdat[infdat$exit > infdat$enter + 1.e-6, ] 
rownames(infdat) <- 1:NROW(infdat)
nrrawd <- NROW(infdat)
##length(unique(infdat$id))
x <- head(infdat) ## OK!
```


```{r agecut}
library(eha)
neo <- age.window(infdat, c(0, 28 / 365))
## events = 2693
```

```{r transform}
source("R/tbl.R")
##neo$enter <- round(365 * neo$enter)
##neo$exit <- round(365 * neo$exit)
##neo <- neo[neo$exit > neo$enter, ] # Remove zero-length spells
neo$enter <- round(365 * neo$enter)
neo$exit <- round(365 * neo$exit)
zero <- neo$exit <= 0 & neo$event == 1
neo$exit[zero] <- 6 / 24
neo <- neo[neo$exit > neo$enter, ]
dneo <- dim(neo)

```


```{r clean}
neo <- neo[neo$parity > 0, ] # Remove missing = -1
neo$parity <- cut(neo$parity, c(0, 1.5, 3.5, 18.5), labels = c("1", "2-3", "4+"))
neo$socStatus[is.na(neo$socStatus)] <- "low"
x <- head(neo) ## OK
tbl(x, caption = "Raw individual data, age in days. First few rows.")
```

Next, *tabulate* data with the aid of the function *toTpch*, found in *eha*, see Table \@ref(tab:tabulate).

```{r tabulate}
tneo <- toTpch(Surv(enter, exit, event) ~  
                   sex + socStatus + illeg + parity, 
               data = neo, cuts = 0:28)
DIMT <- dim(tneo)
X <- head(tneo)
##summary(tneo)
tbl(X, caption = "Same data, tabular form, age in days. First few rows.")
saveRDS(tneo, file = "t_data/tneo.rds")
```

Note how it works: For each combination of *socStatus*, *illeg*, *parity*,
 and *age*, the number of deaths (*event*)
and the *exposure* time (in days) are calculated. The tabulated data frame is 
0.17 per cent in size compared to the original file, and it is also *anonymous*.

The age interval $(0, 28]$ is cut into 28 pieces, (0-1], (1-2], etc., so the underlying statistical survival model is that of a *piecevise constant hazards*.

So, it should not be any problem with public distribution of the tabulated data, but how about the analyses of the two data sets, will there be any differences in results? Not really, see what follows.

# The Cox regression model

On the original data frame, we can run an ordinary Cox regression, see Table 
\@ref(tab:coxreg) for results.

```{r coxreg, results = 'asis'}
fit.cr <- coxreg(Surv(enter, exit, event) ~ 
                                  sex + socStatus + illeg + parity,
                 data = neo) 
dr <- drop1(fit.cr, test = "Chisq")
ltx(fit.cr, dr = dr, label = "tab:coxreg", caption = "Cox regression.")
```

# The tabulated piecewise constant hazards (pch) model

On the tabulated data frame, we get, see Table \@ref(tab:tpchreg).

```{r tpchreg, results = 'asis'}
fit.pch <- tpchreg(oe(event, exposure) ~ 
                                    sex + socStatus + illeg + parity,
                                time = age, data = tneo)
dr2 <- drop1(fit.pch, test = "Chisq")
ltx(fit.pch, dr = dr2, label = "tab:tpchreg", 
    caption = "Piecewise constant proportional hazards regression.")
```

Almost identical results compared to Table \@ref(tab:coxreg). We can compare 
baseline cumulative hazards plots (Figure \@ref(fig:baseplots)):

```{r baseplots, fig.cap = "Comparison.", fig.height = 6}
op <- par(mfrow = c(2, 1), las = 1)
plot(fit.cr, main = "Cox regression", xlab = "Age (days)", ylab = "Cumulative hazard")
plot(fit.pch, fn = "cum", main = "Pch regression", xlab = "Age (days)")
abline(h = 0)
par(op)
```
 
 Almost identical, disregarding differences in smoothness. An advantage with the 
 pch model is that we have access to an estimate of the baseline *hazard* function,
 see Figure \@ref(fig:haz).

```{r haz, fig.cap = "Baseline hazard function.",  fig.height = 4}
par(las = 1)
plot(fit.pch, fn = "haz")
abline(h = 0)
```
 
# Conclusion

There is almost never any disadvantage in analyzing tabular data instead of the original individual-based data, rather the opposite: No loss of information, and the table is *anonymous* and may be freely distributed and published. Further, the table is often just a small fraction in size
compared to the original (Table 1 consists of 391009 rows, Table 2 of 672 rows), resulting in analysis time being much shorter. 
